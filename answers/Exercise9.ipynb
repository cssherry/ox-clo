{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wind and temp is now available in cleanedaverages\n",
      "+-------+-------------+------------------+------------------+\n",
      "|station|     datehour|              temp|              wind|\n",
      "+-------+-------------+------------------+------------------+\n",
      "|   SF15| 2014-01-01:7| 4.964458333333332|0.8799791666666664|\n",
      "|   SF15|2014-05-03:15|27.309583333333336|2.4722083333333336|\n",
      "|   SF15| 2014-05-04:8|21.153750000000002|2.4801249999999997|\n",
      "|   SF15|2014-06-01:14| 33.20849999999999|2.1222833333333333|\n",
      "|   SF15|2014-09-05:10| 25.05479166666667|2.1636041666666666|\n",
      "|   SF15|2014-10-06:21|15.847166666666665|0.4448166666666665|\n",
      "|   SF37|2014-04-03:16|16.678499999999996|3.4063666666666665|\n",
      "|   SF37|2014-07-07:14| 17.74395833333333| 5.301333333333333|\n",
      "|   SF37| 2014-09-05:4|           16.0375|1.7272291666666668|\n",
      "|   SF37| 2014-09-06:8|          17.34625|1.8362708333333329|\n",
      "+-------+-------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "\n",
    "from numpy import array\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "#spark sql setup\n",
    "conf = SparkConf().setAppName(\"wind-sfpd\")\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "\n",
    "# a useful function to parse and clean date/time\n",
    "\n",
    "def date_and_hour(s):\n",
    "    dt = parse(s.replace('?',' '))\n",
    "    hour = dt.hour\n",
    "    return dt.strftime(\"%Y-%m-%d\")+\":\" +str(hour)\n",
    "\n",
    "\n",
    "# start by reading the wind and temperature date\n",
    "\n",
    "df = sqlc.read.format('csv').options(header='true').load('/home/oxclo/datafiles/wind2014/*.csv')\n",
    "\n",
    "tidied = df.rdd.map(lambda r: Row(station = r.Station_ID, datehour =date_and_hour(r.Interval_End_Time), temp=r.Ambient_Temperature_Deg_C, wind=r.Wind_Velocity_Mtr_Sec)).toDF()\n",
    "\n",
    "nonulls = tidied.filter(tidied.temp.isNotNull()).filter(tidied.wind.isNotNull())\n",
    "\n",
    "numbered = nonulls.rdd.map(lambda row: Row(station=row.station, datehour=row.datehour, wind=float(row.wind), temp=float(row.temp))).toDF()\n",
    "\n",
    "averages = numbered.groupBy(['station','datehour']).agg({'temp':'avg', 'wind':'avg'})\n",
    "\n",
    "cleanedaverages = averages.rdd.map(lambda row: Row(station=row.station, datehour=row.datehour, temp=row['avg(temp)'], wind=row['avg(wind)'])).toDF()\n",
    "\n",
    "print(\"wind and temp is now available in cleanedaverages\")\n",
    "\n",
    "cleanedaverages.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we now have incidents by station/hour in incidentcount\n",
      "+-------+-------------+---------+\n",
      "|station|     datehour|incidents|\n",
      "+-------+-------------+---------+\n",
      "|   SF04|2014-12-31:18|        2|\n",
      "|   SF14| 2014-12-29:8|        1|\n",
      "|   SF19|2014-12-26:22|        4|\n",
      "|   SF34| 2014-12-25:0|        1|\n",
      "|   SF37| 2014-12-22:4|        1|\n",
      "|   SF37|2014-12-18:15|        8|\n",
      "|   SF36| 2014-12-18:5|        1|\n",
      "|   SF36|2014-12-16:21|        2|\n",
      "|   SF18|2014-12-16:20|        3|\n",
      "|   SF14| 2014-12-16:1|        2|\n",
      "+-------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "[[1.         0.24453833 0.06069799]\n",
      " [0.24453833 1.         0.24087844]\n",
      " [0.06069799 0.24087844 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# now read the incident data and clean \n",
    "\n",
    "idf = sqlc.read.format('csv').options(header='true').load('/home/oxclo/datafiles/incidents/sfpd.csv')\n",
    "\n",
    "withyx2014 = idf.filter(idf.X.isNotNull()).filter(idf.Y.isNotNull()).filter(idf.Date.contains('2014'))\n",
    "\n",
    "tidy = withyx2014.rdd.map(lambda row: Row(datehour = date_and_hour(row.Date+\" \"+row.Time),yx=[float(row.Y),float(row.X)])).toDF()\n",
    "\n",
    "# need to associate incidents with nearest weather station\n",
    "\n",
    "def locate(l,index,locations):\n",
    "    distance,i = index.query(l)\n",
    "    return locations[i]\n",
    "\n",
    "def map_yx_to_station(yx):\n",
    "    return locate(yx,         spatial.KDTree(array(         [[37.7816834,-122.3887657],        [37.7469112,-122.4821759],        [37.7411022,-120.804151],        [37.4834543,-122.3187302],        [37.7576436,-122.3916382],        [37.7970013,-122.4140409],        [37.748496,-122.4567461],        [37.7288155,-122.4210133],        [37.5839487,-121.9499339],        [37.7157156,-122.4145311],        [37.7329613,-122.5051491],        [37.7575891,-122.3923824],        [37.7521169,-122.4497687]])),\n",
    "        [\"SF18\", \"SF04\", \"SF15\", \"SF17\", \"SF36\", \"SF37\",\\\n",
    "        \"SF07\", \"SF11\", \"SF12\", \"SF14\", \"SF16\", \"SF19\", \"SF34\"] )\n",
    "\n",
    "\n",
    "                  \n",
    "withstations = tidy.rdd.map(lambda row: Row(station=map_yx_to_station(row.yx), datehour=row.datehour)).toDF()\n",
    "\n",
    "withstations.registerTempTable('stationincidents')\n",
    "incidentcount = sqlc.sql(\"select station, datehour, count(1) as incidents from stationincidents group by station, datehour\")\n",
    "\n",
    "print(\"we now have incidents by station/hour in incidentcount\")\n",
    "incidentcount.show(10)\n",
    "\n",
    "\n",
    "# now join the two tables\n",
    "joined = cleanedaverages.join(incidentcount, ['station', 'datehour'], 'outer')\n",
    "\n",
    "# if incident data doesn't exist for that station/datehour, then it is 0\n",
    "zeroed = joined.rdd.map(lambda row: Row(station = row.station, datehour=row.datehour, temp = row.temp, wind = row.wind, incidents = row.incidents if row.incidents  else 0)).toDF()\n",
    "\n",
    "# if temp/wind data doesn't exist for that station/datehour, then we can't use that row\n",
    "final = zeroed.filter(zeroed.temp.isNotNull()).filter(zeroed.wind.isNotNull()).filter(zeroed.temp!=0)\n",
    "\n",
    "# finally apply correlation test\n",
    "vecs = final.rdd.map(lambda row: Vectors.dense([row.temp,row.wind,row.incidents]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "result = Statistics.corr(vecs)\n",
    "labels = ['temp','wind','incidents']\n",
    "sns.heatmap(result, annot = True, xticklabels = labels, yticklabels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
